{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os,time,pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import dump, load\n",
    "from utils import data_process, visualize\n",
    "from utils.utils import train, test, summary, setup_seed\n",
    "from utils.models import Att_CNN, CNN, TFEQ\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1118\n",
      "1     372\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "data,label=[],[]\n",
    "\n",
    "# data.append(np.load('/home/Amin/EQ_Place/code/data/X_2_10.npy', allow_pickle=True))\n",
    "# label.append(np.load('/home/Amin/EQ_Place/code/data/Y_2_10.npy'))\n",
    "data.append(np.load('/home/Amin/EQ_Place/code/data/X_8_28.npy', allow_pickle=True))\n",
    "label.append(np.load('/home/Amin/EQ_Place/code/data/Y_8_28.npy'))\n",
    "\n",
    "data=np.vstack(data)\n",
    "label=np.hstack(label)\n",
    "print(pd.value_counts(label))\n",
    "num=pd.value_counts(label)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###======Settings for different test cases======###\n",
    "SamplingRate = 100 # need to be changed, 25/50/100\n",
    "Duration = 2 # need to be changed, 2/4/10\n",
    "###============###\n",
    "WindowSize = 2 * SamplingRate\n",
    "original_SamplingRate = 100\n",
    "rate = original_SamplingRate/SamplingRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 372/372 [00:00<00:00, 6134.82it/s]\n"
     ]
    }
   ],
   "source": [
    "X_EQ, Y_EQ, Z_EQ = [], [], []\n",
    "for i in tqdm(range(num)):\n",
    "    X = data[i][0]\n",
    "    X_peak = np.where(X == np.max(X))[0][0]\n",
    "    start = X_peak - int(SamplingRate)\n",
    "    end = X_peak + int(SamplingRate) * (Duration - 1)\n",
    "    X_tg, Y_tg, Z_tg = data[i][0][start:end], data[i][1][start:end], data[i][\n",
    "        2][start:end]\n",
    "    X_tg = (X_tg - np.mean(X_tg))\n",
    "    Y_tg = (Y_tg - np.mean(Y_tg)) \n",
    "    Z_tg = (Z_tg - np.mean(Z_tg)) \n",
    "    if len(X_tg) == SamplingRate * Duration:\n",
    "        ## 2 sec sliding window with 1 sec overlap\n",
    "        for j in np.arange(0, len(X_tg) - SamplingRate, SamplingRate):\n",
    "            X_batch = X_tg[j:j + WindowSize]\n",
    "            Y_batch = Y_tg[j:j + WindowSize]\n",
    "            Z_batch = Z_tg[j:j + WindowSize]\n",
    "            if len(X_batch) == WindowSize:\n",
    "                X_EQ.append(X_batch)\n",
    "                Y_EQ.append(Y_batch)\n",
    "                Z_EQ.append(Z_batch)\n",
    "\n",
    "X_EQ, Y_EQ, Z_EQ = np.asarray(X_EQ), np.asarray(Y_EQ), np.asarray(Z_EQ)\n",
    "\n",
    "X_EQ = X_EQ.reshape(int(len(X_EQ) / (Duration - 1)), Duration - 1, WindowSize, 1)\n",
    "Y_EQ = Y_EQ.reshape(int(len(Y_EQ) / (Duration - 1)), Duration - 1, WindowSize, 1)\n",
    "Z_EQ = Z_EQ.reshape(int(len(Z_EQ) / (Duration - 1)), Duration - 1, WindowSize, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1118/1118 [00:01<00:00, 610.23it/s]\n"
     ]
    }
   ],
   "source": [
    "X_HA, Y_HA, Z_HA = [], [], []\n",
    "for i in tqdm(range(num, len(data))):\n",
    "    X, Y, Z = data[i][0], data[i][1], data[i][2]\n",
    "    X_tg = (X - np.mean(X)) / 9.80665\n",
    "    Y_tg = (Y - np.mean(Y)) / 9.80665\n",
    "    Z_tg = (Z - np.mean(Z)) / 9.80665\n",
    "    for j in np.arange(0, 400 - SamplingRate, SamplingRate):\n",
    "        X_batch = X_tg[j:j + WindowSize]\n",
    "        Y_batch = Y_tg[j:j + WindowSize]\n",
    "        Z_batch = Z_tg[j:j + WindowSize]\n",
    "        if len(X_batch) == WindowSize:\n",
    "            X_HA.append(X_batch)\n",
    "            Y_HA.append(Y_batch)\n",
    "            Z_HA.append(Z_batch)\n",
    "\n",
    "X_HA, Y_HA, Z_HA = np.asarray(X_HA), np.asarray(Y_HA), np.asarray(Z_HA)\n",
    "\n",
    "X_HA = X_HA.reshape(X_HA.shape[0], X_HA.shape[1], 1)\n",
    "Y_HA = Y_HA.reshape(Y_HA.shape[0], Y_HA.shape[1], 1)\n",
    "Z_HA = Z_HA.reshape(Z_HA.shape[0], Z_HA.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(X_EQ))\n",
    "X_EQ_train, X_EQ_test, train_index, test_index = train_test_split(X_EQ, indices, test_size=0.995, random_state=42)\n",
    "Y_EQ_train, Y_EQ_test = Y_EQ[train_index], Y_EQ[test_index]\n",
    "Z_EQ_train, Z_EQ_test = Z_EQ[train_index], Z_EQ[test_index]\n",
    "\n",
    "X_EQ_train = X_EQ_train.reshape(X_EQ_train.shape[0]*X_EQ_train.shape[1], X_EQ_train.shape[2], 1)\n",
    "Y_EQ_train = Y_EQ_train.reshape(Y_EQ_train.shape[0]*Y_EQ_train.shape[1], Y_EQ_train.shape[2], 1)\n",
    "Z_EQ_train = Z_EQ_train.reshape(Z_EQ_train.shape[0]*Z_EQ_train.shape[1], Z_EQ_train.shape[2], 1)\n",
    "\n",
    "X_EQ_test = X_EQ_test.reshape(X_EQ_test.shape[0]*X_EQ_test.shape[1], X_EQ_test.shape[2], 1)\n",
    "Y_EQ_test = Y_EQ_test.reshape(Y_EQ_test.shape[0]*Y_EQ_test.shape[1], Y_EQ_test.shape[2], 1)\n",
    "Z_EQ_test = Z_EQ_test.reshape(Z_EQ_test.shape[0]*Z_EQ_test.shape[1], Z_EQ_test.shape[2], 1)\n",
    "\n",
    "indices2 = np.arange(len(X_HA))\n",
    "X_HA_train, X_HA_test, train_index2, test_index2 = train_test_split(X_HA, indices2, test_size=0.995, random_state=42)\n",
    "Y_HA_train, Y_HA_test = Y_HA[train_index2], Y_HA[test_index2]\n",
    "Z_HA_train, Z_HA_test = Z_HA[train_index2], Z_HA[test_index2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 200, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_EQ_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EQ_X_train = np.dstack((X_EQ_train, Y_EQ_train, Z_EQ_train))\n",
    "# HA_X_train = np.dstack((X_HA_train, Y_HA_train, Z_HA_train))\n",
    "\n",
    "# EQ_y_train = np.ones(len(X_EQ_train))\n",
    "# HA_y_train = np.zeros(len(X_HA_train))\n",
    "\n",
    "# X_train = np.vstack((EQ_X_train, HA_X_train))\n",
    "# y_train = np.hstack((EQ_y_train, HA_y_train)).reshape(-1,1)\n",
    "\n",
    "EQ_X_test = np.dstack((X_EQ_test, Y_EQ_test, Z_EQ_test))\n",
    "HA_X_test = np.dstack((X_HA_test, Y_HA_test, Z_HA_test))\n",
    "\n",
    "EQ_y_test = np.ones(len(X_EQ_test))\n",
    "HA_y_test = np.zeros(len(X_HA_test))\n",
    "\n",
    "X_test = np.vstack((EQ_X_test, HA_X_test))\n",
    "y_test = np.hstack((EQ_y_test, HA_y_test)).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic1={'x_train':X_train,'y_train':y_train}\n",
    "\n",
    "np.save(\"data_train.npy\",dic1)\n",
    "\n",
    "dic2={'x_test':X_test,'y_test':y_test}\n",
    "\n",
    "np.save(\"data_test.npy\",dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "\n",
    "x_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "traindata = TensorDataset(x_train, y_train)\n",
    "testdata = TensorDataset(x_test, y_test)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(traindata, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(testdata, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1, Average Loss: 0.3439\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3253, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 2, Average Loss: 0.3295\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3258, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 3, Average Loss: 0.3245\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3306, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 4, Average Loss: 0.3318\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3252, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 5, Average Loss: 0.3294\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3293, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 6, Average Loss: 0.3239\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3350, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 7, Average Loss: 0.3269\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3345, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 8, Average Loss: 0.3261\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3252, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 9, Average Loss: 0.3235\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3268, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 10, Average Loss: 0.3245\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3259, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 11, Average Loss: 0.3237\n",
      "time 0.7 sec:\n",
      "Test set: Average loss: 0.3275, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 12, Average Loss: 0.3244\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3251, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 13, Average Loss: 0.3238\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3246, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 14, Average Loss: 0.3242\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3262, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 15, Average Loss: 0.3236\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3293, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 16, Average Loss: 0.3227\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3232, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 17, Average Loss: 0.3246\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3249, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 18, Average Loss: 0.3255\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3255, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 19, Average Loss: 0.3245\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3205, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 20, Average Loss: 0.3219\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3244, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 21, Average Loss: 0.3164\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3398, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 22, Average Loss: 0.3211\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3112, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 23, Average Loss: 0.3063\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.3158, Accuracy: 1007/1119 (89.99%)\n",
      "\n",
      "Train Epoch: 24, Average Loss: 0.2965\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.2869, Accuracy: 1014/1119 (90.62%)\n",
      "\n",
      "Train Epoch: 25, Average Loss: 0.2804\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.2492, Accuracy: 1021/1119 (91.24%)\n",
      "\n",
      "Train Epoch: 26, Average Loss: 0.2851\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.2360, Accuracy: 1023/1119 (91.42%)\n",
      "\n",
      "Train Epoch: 27, Average Loss: 0.2139\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.1417, Accuracy: 1052/1119 (94.01%)\n",
      "\n",
      "Train Epoch: 28, Average Loss: 0.1969\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.4569, Accuracy: 1119/1119 (100.00%)\n",
      "\n",
      "Train Epoch: 29, Average Loss: 0.1739\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.1334, Accuracy: 1059/1119 (94.64%)\n",
      "\n",
      "Train Epoch: 30, Average Loss: 0.1326\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.0717, Accuracy: 1085/1119 (96.96%)\n",
      "\n",
      "Train Epoch: 31, Average Loss: 0.1277\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.1724, Accuracy: 1062/1119 (94.91%)\n",
      "\n",
      "Train Epoch: 32, Average Loss: 0.0921\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.0747, Accuracy: 1084/1119 (96.87%)\n",
      "\n",
      "Train Epoch: 33, Average Loss: 0.1176\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.0646, Accuracy: 1088/1119 (97.23%)\n",
      "\n",
      "Train Epoch: 34, Average Loss: 0.0821\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.0498, Accuracy: 1097/1119 (98.03%)\n",
      "\n",
      "Train Epoch: 35, Average Loss: 0.0798\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.0342, Accuracy: 1117/1119 (99.82%)\n",
      "\n",
      "Train Epoch: 36, Average Loss: 0.0578\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.0302, Accuracy: 1104/1119 (98.66%)\n",
      "\n",
      "Train Epoch: 37, Average Loss: 0.0508\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.0193, Accuracy: 1108/1119 (99.02%)\n",
      "\n",
      "Train Epoch: 38, Average Loss: 0.0551\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.0848, Accuracy: 1094/1119 (97.77%)\n",
      "\n",
      "Train Epoch: 39, Average Loss: 0.0497\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.0178, Accuracy: 1108/1119 (99.02%)\n",
      "\n",
      "Train Epoch: 40, Average Loss: 0.0963\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.1077, Accuracy: 1077/1119 (96.25%)\n",
      "\n",
      "Train Epoch: 41, Average Loss: 0.0495\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.0299, Accuracy: 1104/1119 (98.66%)\n",
      "\n",
      "Train Epoch: 42, Average Loss: 0.0306\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.0104, Accuracy: 1116/1119 (99.73%)\n",
      "\n",
      "Train Epoch: 43, Average Loss: 0.0306\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.0854, Accuracy: 1094/1119 (97.77%)\n",
      "\n",
      "Train Epoch: 44, Average Loss: 0.1376\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.1139, Accuracy: 1096/1119 (97.94%)\n",
      "\n",
      "Train Epoch: 45, Average Loss: 0.1477\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.1073, Accuracy: 1063/1119 (95.00%)\n",
      "\n",
      "Train Epoch: 46, Average Loss: 0.0932\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.0428, Accuracy: 1118/1119 (99.91%)\n",
      "\n",
      "Train Epoch: 47, Average Loss: 0.0670\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.1432, Accuracy: 1078/1119 (96.34%)\n",
      "\n",
      "Train Epoch: 48, Average Loss: 0.0432\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.0285, Accuracy: 1106/1119 (98.84%)\n",
      "\n",
      "Train Epoch: 49, Average Loss: 0.0330\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.0160, Accuracy: 1112/1119 (99.37%)\n",
      "\n",
      "Train Epoch: 50, Average Loss: 0.0299\n",
      "time 0.8 sec:\n",
      "Test set: Average loss: 0.0100, Accuracy: 1116/1119 (99.73%)\n",
      "\n",
      "CPU times: user 46.8 s, sys: 821 ms, total: 47.6 s\n",
      "Wall time: 44.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "setup_seed(42)\n",
    "model = TFEQ(channel=3, time_in=200).cuda()\n",
    "L = []\n",
    "test_hist = []\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(1, 50 + 1):\n",
    "    start = time.time()\n",
    "    loss = train(model, train_loader, optimizer, epoch)\n",
    "    L.append(loss)\n",
    "    print(\"time {:.1f} sec:\".format(time.time() - start))\n",
    "    acc = test(model, test_loader)\n",
    "    test_hist.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      " [[1007    3]\n",
      " [   0  109]]\n",
      "Roc score: 0.9985\n",
      "F1 score: 0.9864\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1010\n",
      "           1       0.97      1.00      0.99       109\n",
      "\n",
      "    accuracy                           1.00      1119\n",
      "   macro avg       0.99      1.00      0.99      1119\n",
      "weighted avg       1.00      1.00      1.00      1119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = []\n",
    "y_test = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data = data.cuda()\n",
    "        output = model(data)\n",
    "        pred = output.max(1, keepdim=True)[1]  \n",
    "        pred = pred.cpu().numpy().squeeze()\n",
    "        y_pred.append(pred)\n",
    "        y_test.append(target.numpy().squeeze())\n",
    "y_pred=np.hstack(y_pred)\n",
    "y_test=np.hstack(y_test)\n",
    "\n",
    "summary(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 477.844,
   "position": {
    "height": "40px",
    "left": "1119px",
    "right": "20px",
    "top": "60px",
    "width": "560px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
