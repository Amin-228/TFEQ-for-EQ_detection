{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from One_hot_encoder import One_hot_encoder\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATANAME = 'PEMS-BAY'\n",
    "TIMESTEP_IN = 12\n",
    "TIMESTEP_OUT = 12 # It can be overrided by param_stgcn3\n",
    "N_NODE = 325\n",
    "CHANNEL = 1\n",
    "BATCHSIZE = 64\n",
    "LEARN = 0.001\n",
    "EPOCH = 200\n",
    "PATIENCE = 10\n",
    "OPTIMIZER = 'Adam'\n",
    "# OPTIMIZER = 'RMSprop'\n",
    "# LOSS = 'MSE'\n",
    "LOSS = 'MAE'\n",
    "TRAINRATIO = 0.8 # TRAIN + VAL\n",
    "TRAINVALSPLIT = 0.125 # val_ratio = 0.8 * 0.125 = 0.1\n",
    "FLOWPATH = '../PEMSBAY/pems-bay.h5'\n",
    "ADJPATH = '../PEMSBAY/W_pemsbay.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        '''\n",
    "        Q: [batch_size, n_heads, T(Spatial) or N(Temporal), N(Spatial) or T(Temporal), d_k]\n",
    "        K: [batch_size, n_heads, T(Spatial) or N(Temporal), N(Spatial) or T(Temporal), d_k]\n",
    "        V: [batch_size, n_heads, T(Spatial) or N(Temporal), N(Spatial) or T(Temporal), d_k]\n",
    "        attn_mask: [batch_size, n_heads, seq_len, seq_len] 可能没有\n",
    "        '''\n",
    "        B, n_heads, len1, len2, d_k = Q.shape \n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) \n",
    "        # scores : [batch_size, n_heads, T(Spatial) or N(Temporal), N(Spatial) or T(Temporal), N(Spatial) or T(Temporal)]\n",
    "        # scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is True.\n",
    "        \n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V) # [batch_size, n_heads, T(Spatial) or N(Temporal), N(Spatial) or T(Temporal), d_k]]\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(TMultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "            \n",
    "\n",
    "        self.W_V = nn.Linear(self.embed_size, self.head_dim * self.heads, bias=False)\n",
    "        self.W_K = nn.Linear(self.embed_size, self.head_dim * self.heads, bias=False)\n",
    "        self.W_Q = nn.Linear(self.embed_size, self.head_dim * self.heads, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "    def forward(self, input_Q, input_K, input_V):\n",
    "        '''\n",
    "        input_Q: [batch_size, N, T, C]\n",
    "        input_K: [batch_size, N, T, C]\n",
    "        input_V: [batch_size, N, T, C]\n",
    "        attn_mask: [batch_size, seq_len, seq_len]\n",
    "        '''\n",
    "        B, N, T, C = input_Q.shape\n",
    "        # [B, N, T, C] --> [B, N, T, h * d_k] --> [B, N, T, h, d_k] --> [B, h, N, T, d_k]\n",
    "        Q = self.W_Q(input_Q).view(B, N, T, self.heads, self.head_dim).permute(0, 3, 1, 2, 4) # Q: [B, h, N, T, d_k]\n",
    "        K = self.W_K(input_K).view(B, N, T, self.heads, self.head_dim).permute(0, 3, 1, 2, 4)  # K: [B, h, N, T, d_k]\n",
    "        V = self.W_V(input_V).view(B, N, T, self.heads, self.head_dim).permute(0, 3, 1, 2, 4)  # V: [B, h, N, T, d_k]\n",
    "\n",
    "        # attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\n",
    "\n",
    "        # context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]\n",
    "        context = ScaledDotProductAttention()(Q, K, V) #[B, h, N, T, d_k]\n",
    "        context = context.permute(0, 2, 3, 1, 4) #[B, N, T, h, d_k]\n",
    "        context = context.reshape(B, N, T, self.heads * self.head_dim) # [B, N, T, C]\n",
    "        # context = context.transpose(1, 2).reshape(batch_size, -1, n_heads * d_v) # context: [batch_size, len_q, n_heads * d_v]\n",
    "        output = self.fc_out(context) # [batch_size, len_q, d_model]\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, pe_length, heads ,forward_expansion, gpu, dropout):\n",
    "        super(Transformer_EncoderBlock, self).__init__()\n",
    "        \n",
    "        # Temporal embedding One hot\n",
    "        self.pe_length = pe_length\n",
    "#         self.one_hot = One_hot_encoder(embed_size, pe_length)          # temporal embedding by one-hot\n",
    "        self.temporal_embedding = nn.Embedding(pe_length, embed_size)  # temporal embedding  by nn.Embedding\n",
    "        self.attention = TMultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gpu = gpu\n",
    "    def forward(self, value, key, query):\n",
    "        B, N, T, C = query.shape\n",
    "        \n",
    "#         D_T = self.one_hot(t, N, T)                          # temporal embedding by one-hot\n",
    "        D_T = self.temporal_embedding(torch.arange(0, T).to(self.gpu))    # temporal embedding  by nn.Embedding\n",
    "        D_T = D_T.expand(B, N, T, C)\n",
    "\n",
    "        # temporal embedding + query。 (concatenated or add here is add)\n",
    "        query = query + D_T  \n",
    "#         print('query + D_T shape:',query.shape)\n",
    "\n",
    "        attention = self.attention(query, query, query)\n",
    "\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTransformer_EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_size, pe_length, heads ,forward_expansion, gpu, dropout):\n",
    "        super(TTransformer_EncoderLayer, self).__init__()\n",
    "#         self.STransformer = STransformer(embed_size, heads, adj, cheb_K, dropout, forward_expansion)\n",
    "        self.Transformer_EncoderBlock = Transformer_EncoderBlock(embed_size, pe_length, heads ,forward_expansion, gpu, dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, value, key, query):\n",
    "    # value,  key, query: [N, T, C] [B, N, T, C]\n",
    "        # Add skip connection,run through normalization and finally dropout\n",
    "        x1 = self.norm1(self.Transformer_EncoderBlock(value, key, query) + query) #(B, N, T, C)\n",
    "        x2 = self.dropout(x1) \n",
    "\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # 堆叠多层 ST-Transformer Block\n",
    "    def __init__(\n",
    "        self,embed_size,num_layers,pe_length,heads,forward_expansion,gpu,dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.gpu = gpu\n",
    "        self.layers = nn.ModuleList([ TTransformer_EncoderLayer(embed_size, pe_length, heads ,forward_expansion, gpu, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):     \n",
    "        # x: [N, T, C]  [B, N, T, C]\n",
    "        out = self.dropout(x)      \n",
    "        # In the Encoder the query, key, value are all the same.\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out)\n",
    "        return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T_Transformer_block(nn.Module):\n",
    "    def __init__(self,embed_size,num_layers,pe_length,heads,forward_expansion, gpu,dropout):\n",
    "        super(T_Transformer_block, self).__init__()\n",
    "        self.encoder = Encoder(embed_size,num_layers,pe_length,heads,forward_expansion,gpu,dropout)\n",
    "        self.gpu = gpu\n",
    "\n",
    "    def forward(self, src): \n",
    "        ## scr: [N, T, C]   [B, N, T, C]\n",
    "        enc_src = self.encoder(src) \n",
    "        return enc_src # [B, N, T, C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T_Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, embed_size, pe_length, num_layers, timestep_in, timestep_out, heads, forward_expansion, gpu, dropout):        \n",
    "        super(T_Transformer, self).__init__()\n",
    "\n",
    "        self.forward_expansion = forward_expansion\n",
    "        # C --> expand  --> hidden dim (embed_size)\n",
    "        self.conv1 = nn.Conv2d(in_channels, embed_size, 1)\n",
    "        \n",
    "        self.T_Transformer_block = T_Transformer_block(embed_size, num_layers, pe_length,heads, forward_expansion, gpu, dropout)\n",
    "\n",
    "        # Reduce the temporal dimension。  timestep_in --> out_timestep_in\n",
    "        self.conv2 = nn.Conv2d(timestep_in, timestep_out, 1)  \n",
    "        # Reduce the C dimension，to 1。\n",
    "        self.conv3 = nn.Conv2d(embed_size, 1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.Tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input x shape  [B, T, N, C]  C  = CHANNEL = 1\n",
    "        # C: channel。  N:nodes。  T:time\n",
    "        \n",
    "        x = x.permute(0,3,2,1)   # [B, T, N, C] -> [B, C, N, T]\n",
    "        input_Transformer = self.conv1(x)     #    x shape[B, C, N, T]   --->    input_Transformer shape： [B, H = embed_size = 64, N, T] \n",
    "        input_Transformer = input_Transformer.permute(0, 2, 3, 1)    # [B, H, N, T] [B, N, T, H]\n",
    "        output_Transformer = self.T_Transformer_block(input_Transformer)  # [B, N, T, H]\n",
    "        output_Transformer = output_Transformer.permute(0, 2, 1, 3)   # [B, N, T, H] -> [B, T, N, H]\n",
    "        \n",
    "        out = self.relu(self.conv2(output_Transformer))    #   [B, T, N, H] ->  [B, T, N, C=1]         \n",
    "        out = out.permute(0, 3, 2, 1)           # [B, T, N, C=1]  ->  [B, C=1, N, T]\n",
    "        out = self.conv3(out)                   # \n",
    "        out = out.permute(0, 3, 2, 1)           # [B, C=1, N, T] -> [B,T,N,1]\n",
    "\n",
    "        return out      #[B, TIMESTEP_OUT, N, C]   C = CHANNEL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_params(model_name, model):\n",
    "    param_count=0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            param_count += param.numel()\n",
    "    print(f'{model_name}, {param_count} trainable parameters in total.')\n",
    "    return  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = sys.argv[-1] if len(sys.argv) == 2 else '5'\n",
    "device = torch.device(\"cuda:{}\".format(GPU)) if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_Transformer, 282333 trainable parameters in total.\n"
     ]
    }
   ],
   "source": [
    "model = T_Transformer(in_channels=CHANNEL,\n",
    "                      embed_size=64,\n",
    "                      pe_length=TIMESTEP_IN,\n",
    "                      num_layers=1,\n",
    "                      timestep_in=TIMESTEP_IN,\n",
    "                      timestep_out=TIMESTEP_OUT,\n",
    "                      heads=8,\n",
    "                      forward_expansion=32,\n",
    "                      gpu=device,\n",
    "                      dropout=0).to(device)\n",
    "print_params('T_Transformer', model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "├─Conv2d: 1-1                                           [-1, 64, 325, 12]         128\n",
      "├─T_Transformer_block: 1-2                              [-1, 325, 12, 64]         --\n",
      "|    └─Encoder: 2-1                                     [-1, 325, 12, 64]         --\n",
      "|    |    └─Dropout: 3-1                                [-1, 325, 12, 64]         --\n",
      "├─Conv2d: 1-3                                           [-1, 12, 325, 64]         156\n",
      "├─ReLU: 1-4                                             [-1, 12, 325, 64]         --\n",
      "├─Conv2d: 1-5                                           [-1, 1, 325, 12]          65\n",
      "=========================================================================================================\n",
      "Total params: 349\n",
      "Trainable params: 349\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 4.33\n",
      "=========================================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.84\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 3.85\n",
      "=========================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "├─Conv2d: 1-1                                           [-1, 64, 325, 12]         128\n",
       "├─T_Transformer_block: 1-2                              [-1, 325, 12, 64]         --\n",
       "|    └─Encoder: 2-1                                     [-1, 325, 12, 64]         --\n",
       "|    |    └─Dropout: 3-1                                [-1, 325, 12, 64]         --\n",
       "├─Conv2d: 1-3                                           [-1, 12, 325, 64]         156\n",
       "├─ReLU: 1-4                                             [-1, 12, 325, 64]         --\n",
       "├─Conv2d: 1-5                                           [-1, 1, 325, 12]          65\n",
       "=========================================================================================================\n",
       "Total params: 349\n",
       "Trainable params: 349\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 4.33\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 3.84\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 3.85\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, (TIMESTEP_IN, N_NODE, CHANNEL), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFst(nn.Module):\n",
    "    def __init__(self, device, channel=1, dmodel=64, nhead=8,dim_feedforward = 32, num_layers=1, num_nodes=325, time_in=12):\n",
    "        super(TFst, self).__init__()\n",
    "        self.input_embedding = nn.Linear(channel, dmodel)\n",
    "        self.spatial_encoder_layer = nn.TransformerEncoderLayer(dmodel, nhead, dim_feedforward)\n",
    "        self.spatial_encoder = nn.TransformerEncoder(self.spatial_encoder_layer, num_layers)\n",
    "        self.temporal_encoder_layer = nn.TransformerEncoderLayer(dmodel, nhead, dim_feedforward)\n",
    "        self.temporal_encoder = nn.TransformerEncoder(self.temporal_encoder_layer, num_layers)\n",
    "        self.decoder = nn.Linear(dmodel, 1)\n",
    "        self.spatial_pos = torch.arange(0, num_nodes).to(device=device)\n",
    "        self.spatial_pe = nn.Embedding(num_nodes, dmodel)\n",
    "        self.temporal_pos = torch.arange(0, time_in).to(device=device)\n",
    "        self.temporal_pe = nn.Embedding(time_in, dmodel)\n",
    "        self.device = device\n",
    "        self.output_map = nn.Linear(time_in, time_in)     # multi-step prediction\n",
    "#         self.output_map = nn.Linear(time_in, 1)     # single-step prediction\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input [B, T, N, C]\n",
    "#         x = x.permute(0, 2, 3, 1)\n",
    "        b, t, n, c = x.shape\n",
    "        x = self.input_embedding(x)   # [B, T, N, C]  -> [B, T, N, H]\n",
    "\n",
    "#         # spatial transformer\n",
    "#         x = x.flatten(start_dim=0, end_dim=1)\n",
    "#         s_pe = self.spatial_pe(self.spatial_pos).expand(b*t, n, -1)\n",
    "#         x = x + s_pe\n",
    "#         x = self.spatial_encoder(x)\n",
    "#         x = x.reshape(b, t, n, -1)\n",
    "\n",
    "        # temporal transformer\n",
    "        x = x.permute(0, 2, 1, 3)  # [B, T, N, H] -> [B, N, T, H]\n",
    "        x = x.flatten(start_dim=0, end_dim=1)\n",
    "        t_pe = self.temporal_pe(self.temporal_pos).expand(b*n, t, -1)\n",
    "        x = x + t_pe\n",
    "        x = self.temporal_encoder(x)\n",
    "        x = x.reshape(b, n, t, -1)\n",
    "\n",
    "        x = self.decoder(x)         # [B, N, T, C]\n",
    "        x = x.permute(0, 3, 1, 2)   # [B, C, N, T]\n",
    "        x = self.output_map(x)\n",
    "        x = x.permute(0, 3, 2, 1)   # [B, T, N, C]\n",
    "\n",
    "#         x = x.permute(0, 2, 1, 3)   # [B, T, N, C]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─Linear: 1-1                                 [-1, 12, 325, 64]         128\n",
      "├─Embedding: 1-2                              [-1, 64]                  768\n",
      "├─TransformerEncoder: 1-3                     [-1, 12, 64]              --\n",
      "|    └─ModuleList: 2                          []                        --\n",
      "|    |    └─TransformerEncoderLayer: 3-1      [-1, 12, 64]              21,088\n",
      "├─Linear: 1-4                                 [-1, 325, 12, 1]          65\n",
      "├─Linear: 1-5                                 [-1, 1, 325, 12]          156\n",
      "===============================================================================================\n",
      "Total params: 22,205\n",
      "Trainable params: 22,205\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.06\n",
      "===============================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.98\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 2.08\n",
      "===============================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "├─Linear: 1-1                                 [-1, 12, 325, 64]         128\n",
       "├─Embedding: 1-2                              [-1, 64]                  768\n",
       "├─TransformerEncoder: 1-3                     [-1, 12, 64]              --\n",
       "|    └─ModuleList: 2                          []                        --\n",
       "|    |    └─TransformerEncoderLayer: 3-1      [-1, 12, 64]              21,088\n",
       "├─Linear: 1-4                                 [-1, 325, 12, 1]          65\n",
       "├─Linear: 1-5                                 [-1, 1, 325, 12]          156\n",
       "===============================================================================================\n",
       "Total params: 22,205\n",
       "Trainable params: 22,205\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.06\n",
       "===============================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 1.98\n",
       "Params size (MB): 0.08\n",
       "Estimated Total Size (MB): 2.08\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFst(device, channel=CHANNEL, num_nodes=N_NODE, time_in=TIMESTEP_IN).to(device)\n",
    "summary(model, (TIMESTEP_IN, N_NODE, CHANNEL), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
